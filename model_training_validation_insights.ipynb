{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Odor Strength Prediction: Model Training, Validation & Insights\n",
    "\n",
    "This notebook provides a the training, hyperparameter optimization, validation and interpretation of machine learning models for predicting odor strength from molecular structures. It includes:\n",
    "\n",
    "1. **Load Dataset**: Examination of the curated odor strength dataset\n",
    "2. **Model Training**: Hyperparameter optimization for various encoder-predictor combinations\n",
    "3. **Performance Evaluation**: Comparison of direct (directly predicting odor strength) vs. indirect (first predicting presence of odor, then strength for odorous molecules) prediction approaches and several combinations of different molecular representations and predictive algorithms\n",
    "4. **Feature Importance**: SHAP analysis to understand model predictions\n",
    "5. **External Validation**: Testing on independent Keller 2016 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tested with python==3.12.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the notebook dataset_curation.ipynb or python script dataset_curation.py first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_odor_strength = pd.read_csv('data/df_odor_strength.csv')\n",
    "groups = pd.read_csv('data/odor_strength_groups.csv', index_col=0).values.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### see additional notebook dataset_analysis.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training with Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from optuna.study import MaxTrialsCallback\n",
    "from optuna.trial import TrialState\n",
    "import os\n",
    "\n",
    "\n",
    "from models.odor_strength_module import OdorStrengthModule, OdorStrengthModuleHyperparameterOptimizationWrapper\n",
    "from models.predictors import Average, LogisticRegressionPredictor, RandomForestPredictor, XGBoostPredictor, MLP_Predictor, ChemPropMPNNPredictor, ChemeleonPredictor, CoralPredictor\n",
    "from models.molecule_encoder import NativeEncoder, MorganFp, RDKitFp, TopologicalTorsionFp, AtomPairFp, MACCSKeysFp, RDKitDescriptors, ChemBerta\n",
    "\n",
    "from hyperparameter_optimization.hyperparameter_optimizer import HyperparameterOptimizer\n",
    "from hyperparameter_optimization.hyperparameter_spaces import HyperparameterSpaces\n",
    "from utility.metrics import Metrics\n",
    "from utility.stratified_group_split import stratified_group_train_test_split\n",
    "\n",
    "hyperparameter_spaces = HyperparameterSpaces()\n",
    "\n",
    "STORAGE_FOLDER = 'sqlite:///hyperparameter_optimization/hp_opt_dbs/'\n",
    "if not os.path.exists('hyperparameter_optimization/hp_opt_dbs/'):\n",
    "    os.makedirs('hyperparameter_optimization/hp_opt_dbs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_odor_strength['canonical_smiles'].values\n",
    "y = df_odor_strength['numerical_strength'].values\n",
    "X_train, _, y_train, _, groups_train, _ = stratified_group_train_test_split(X, y, groups, random_state=42)\n",
    "X_train = X_train.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Model Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odor_strength_module = OdorStrengthModule(\n",
    "    molecule_encoder=MorganFp(),\n",
    "    odor_strength_predictor=RandomForestPredictor()\n",
    ")\n",
    "\n",
    "odor_strength_module.evaluate_kFold(X_train, y_train, groups=groups_train, n_splits=5, metric=Metrics().calculate_mse_macro, plot=True, show_wrong_pred=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_REPEATS = 1\n",
    "N_FOLDS = 2\n",
    "N_TRIALS = 1\n",
    "LIMIT = 100\n",
    "\n",
    "callback = MaxTrialsCallback(\n",
    "    n_trials=LIMIT,\n",
    "    states=(TrialState.COMPLETE, TrialState.PRUNED),\n",
    ")\n",
    "\n",
    "class OdorStrengthHyperparameterOptimizer:\n",
    "    \"\"\"\n",
    "    A hyperparameter optimizer specifically designed for odor strength prediction models.\n",
    "    \n",
    "    This class provides a structured approach to hyperparameter optimization for combinations\n",
    "    of molecular encoders and odor strength predictors using Optuna optimization framework.\n",
    "    \n",
    "    Attributes:\n",
    "        encoder (object): Molecular encoder class to use for molecule representation\n",
    "        predictor (object): Predictor class to use for odor strength prediction\n",
    "        n_trials (int): Number of optimization trials to run\n",
    "        evaluation_metric (Callable): Metric function to optimize\n",
    "        groups (np.ndarray): Group labels for stratified cross-validation\n",
    "        n_cv_splits (int): Number of cross-validation splits\n",
    "        n_cv_repeats (int): Number of cross-validation repeats\n",
    "        load_if_exists (bool): Whether to load existing study if available\n",
    "        predictor_hyperparameter_space_name_prefix (str): Prefix for hyperparameter space function names\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            encoder: object,\n",
    "            predictor: object,\n",
    "            n_trials: int,\n",
    "            evaluation_metric: Callable[[np.ndarray, np.ndarray], float],\n",
    "            groups: np.ndarray = groups,\n",
    "            n_cv_splits: int = 5,\n",
    "            n_cv_repeats: int = 5,\n",
    "            load_if_exists: bool = True,\n",
    "            predictor_hyperparameter_space_name_prefix: str = 'hyperparameter_space_'\n",
    "            ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the hyperparameter optimizer.\n",
    "        \n",
    "        Args:\n",
    "            encoder: Molecular encoder class for molecular representation\n",
    "            predictor: Predictor class for odor strength prediction\n",
    "            n_trials: Number of optimization trials to perform\n",
    "            evaluation_metric: Function to evaluate model performance\n",
    "            groups: Group labels for stratified cross-validation splits\n",
    "            n_cv_splits: Number of cross-validation folds (default: 5)\n",
    "            n_cv_repeats: Number of cross-validation repetitions (default: 5)\n",
    "            load_if_exists: Whether to resume existing optimization study (default: True)\n",
    "            predictor_hyperparameter_space_name_prefix: Prefix for hyperparameter space function names\n",
    "        \"\"\"\n",
    "        self.encoder = encoder\n",
    "        self.predictor = predictor\n",
    "        self.n_trials = n_trials\n",
    "        self.load_if_exists = load_if_exists\n",
    "        self.evaluation_metric = evaluation_metric\n",
    "        self.groups = groups\n",
    "        self.n_cv_splits = n_cv_splits\n",
    "        self.n_cv_repeats = n_cv_repeats\n",
    "        self.predictor_hyperparameter_space_name_prefix = predictor_hyperparameter_space_name_prefix\n",
    "\n",
    "    def evaluation_function(self, odor_strength_module: object, X: list[str], y: np.ndarray) -> tuple[float, dict]:\n",
    "        \"\"\"\n",
    "        Evaluate the performance of an odor strength module using cross-validation.\n",
    "        \n",
    "        Args:\n",
    "            odor_strength_module: Configured odor strength prediction module\n",
    "            X: List of molecular SMILES strings\n",
    "            y: Target values (numeric odor strength labels)\n",
    "            \n",
    "        Returns:\n",
    "            tuple: Mean performance score and detailed evaluation results\n",
    "        \"\"\"\n",
    "        evaluation_result = odor_strength_module.evaluate_kFold(X, y, metric=self.evaluation_metric, n_splits=self.n_cv_splits, n_repeats=self.n_cv_repeats, real_time_evaluation=True, groups=self.groups)\n",
    "        return evaluation_result[0], evaluation_result[3]\n",
    "\n",
    "\n",
    "    def hyperparameter_space(self, trial: optuna.trial.Trial) -> dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Define the hyperparameter search space for encoder-predictor combinations.\n",
    "        \n",
    "        Args:\n",
    "            trial: Optuna trial object for hyperparameter sampling\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary containing encoder and predictor hyperparameters\n",
    "        \"\"\"\n",
    "        hyperparameters = {\n",
    "            'encoder_name': self.encoder.__name__,\n",
    "            'predictor_name': self.predictor.__name__,\n",
    "        }\n",
    "        encoder_function = getattr(hyperparameter_spaces, 'hyperparameter_space_' + self.encoder.__name__)\n",
    "        hyperparameters['hp_molecule_encoder'] = encoder_function(trial)\n",
    "        predictor_function = getattr(hyperparameter_spaces, self.predictor_hyperparameter_space_name_prefix + self.predictor.__name__)\n",
    "        hyperparameters['hp_odor_strength_predictor'] = predictor_function(trial)\n",
    "        return hyperparameters\n",
    "\n",
    "    def optimize_study(\n",
    "            self,\n",
    "            X: list[str],\n",
    "            y: np.ndarray,\n",
    "            direction: str,\n",
    "            study_name: str,\n",
    "            storage_path: str,\n",
    "            pruner: optuna.pruners.BasePruner,\n",
    "            pruner_tolerance: float,\n",
    "            n_repeats: int,\n",
    "            **optimize_kwargs\n",
    "            ) -> optuna.Study:\n",
    "        \"\"\"\n",
    "        Run hyperparameter optimization study.\n",
    "        \n",
    "        Args:\n",
    "            X: List of molecular SMILES strings for training\n",
    "            y: Target values for training\n",
    "            direction: Optimization direction ('minimize' or 'maximize')\n",
    "            study_name: Name for the optimization study\n",
    "            storage_path: Path to store optimization results\n",
    "            pruner: Optuna pruner for early stopping of unpromising trials\n",
    "            pruner_tolerance: Tolerance threshold for not pruning trials to the best-performing trial\n",
    "            n_repeats: Number of evaluation repeats for robustness\n",
    "            **optimize_kwargs: Additional arguments for optimization\n",
    "            \n",
    "        Returns:\n",
    "            optuna.Study: Completed optimization study object\n",
    "        \"\"\"\n",
    "        study = optuna.create_study(\n",
    "            direction=direction,\n",
    "            study_name=study_name,\n",
    "            storage=storage_path,\n",
    "            load_if_exists=self.load_if_exists,\n",
    "            pruner=pruner\n",
    "        )\n",
    "        hp_opt = HyperparameterOptimizer(OdorStrengthModuleHyperparameterOptimizationWrapper, study)\n",
    "        hp_opt.optimize(\n",
    "            X,\n",
    "            y,\n",
    "            hyperparameter_space=self.hyperparameter_space,\n",
    "            evaluation_function=self.evaluation_function,\n",
    "            n_trials=self.n_trials,\n",
    "            n_repeats=n_repeats,\n",
    "            pruner_tolerance=pruner_tolerance,\n",
    "            **optimize_kwargs\n",
    "        )\n",
    "        return study\n",
    "\n",
    "encoders = [\n",
    "    NativeEncoder,\n",
    "    RDKitDescriptors,\n",
    "    MorganFp,\n",
    "    RDKitFp,\n",
    "    TopologicalTorsionFp,\n",
    "    AtomPairFp,\n",
    "    MACCSKeysFp,\n",
    "    ChemBerta\n",
    "]\n",
    "predictors = [\n",
    "    RandomForestPredictor,\n",
    "    XGBoostPredictor,\n",
    "    MLP_Predictor,\n",
    "    LogisticRegressionPredictor,\n",
    "    CoralPredictor,\n",
    "    ChemPropMPNNPredictor,\n",
    "    ChemeleonPredictor,\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization Functions\n",
    "\n",
    "Models for the following steps were optimized:\n",
    "\n",
    "1. **Direct Approach**: Directly predict odor strength values (0-3 scale)\n",
    "2. **Indirect First Step**: Predict odor presence/absence (binary classification)\n",
    "3. **Indirect Second Step**: Predict strength for odorous molecules only (regression on subset (1-3 scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compatibility_check(encoder_name: str, predictor_name: str, w_regression: bool = True) -> bool:\n",
    "    \"\"\"\n",
    "    Check compatibility between molecular encoders and predictors.\n",
    "    \n",
    "    Some predictors require specific types of input features. This function ensures\n",
    "    that incompatible combinations are skipped during optimization.\n",
    "    \n",
    "    Args:\n",
    "        encoder_name (str): Name of the molecular encoder\n",
    "        predictor_name (str): Name of the predictor model\n",
    "        w_regression (bool): Whether regression predictors should be included\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if the combination is compatible, False otherwise\n",
    "    \"\"\"\n",
    "    check_list = ['Average', 'ChemPropMPNNPredictor', 'ChemeleonPredictor']\n",
    "    check_1 = (encoder_name == 'NativeEncoder' and predictor_name not in check_list)\n",
    "    check_2 = (encoder_name != 'NativeEncoder' and predictor_name in check_list)\n",
    "    if  check_1 or check_2:\n",
    "        print(f'Skipping {encoder_name} with {predictor_name} due to incompatibility.')\n",
    "        return False\n",
    "    elif not w_regression and predictor_name == 'CoralPredictor':\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def optimize_hyperparameters_direct_approach(encoder: object, predictor: object) -> None:\n",
    "    \"\"\"\n",
    "    Optimize hyperparameters using direct odor strength prediction approach.\n",
    "    \n",
    "    The encoder-predictor models directly predicts odor strength values (0-3 scale) from SMILES strings.\n",
    "    \n",
    "    Args:\n",
    "        encoder (object): Molecular encoder class for molecular representation\n",
    "        predictor (object): Predictor class for odor strength prediction\n",
    "    \"\"\"\n",
    "    X = df_odor_strength['canonical_smiles'].values\n",
    "    y = df_odor_strength['numerical_strength'].values\n",
    "    X_train, _, y_train, _, groups_train, _ = stratified_group_train_test_split(X, y, groups, random_state=42)\n",
    "    X_train = X_train.tolist()\n",
    "    try:\n",
    "        encoder_name = encoder.__name__\n",
    "        predictor_name = predictor.__name__\n",
    "        if compatibility_check(encoder_name, predictor_name, w_regression=True):\n",
    "            print(f'Optimizing hyperparameters: {encoder_name} with {predictor_name}')\n",
    "            OdorStrengthHyperparameterOptimizer(\n",
    "                encoder,\n",
    "                predictor,\n",
    "                n_trials=N_TRIALS,\n",
    "                evaluation_metric=Metrics().calculate_mse_macro,\n",
    "                groups=groups_train,\n",
    "                n_cv_splits=N_FOLDS,\n",
    "                n_cv_repeats=1,\n",
    "                load_if_exists=True,\n",
    "                predictor_hyperparameter_space_name_prefix='hyperparameter_space_'\n",
    "                ).optimize_study(\n",
    "                    X=X_train,\n",
    "                    y=y_train,\n",
    "                    direction='minimize',\n",
    "                    study_name=f'{encoder.__name__}_{predictor.__name__}_direct',\n",
    "                    storage_path=STORAGE_FOLDER + f'{encoder.__name__}_{predictor.__name__}_direct.db',\n",
    "                    pruner=optuna.pruners.PercentilePruner(25, n_startup_trials=3),\n",
    "                    pruner_tolerance=0.02,\n",
    "                    n_repeats=N_REPEATS,\n",
    "                    callbacks=[callback]\n",
    "                    )\n",
    "    except Exception as e:\n",
    "        print(f'Error with {encoder_name} and {predictor_name}. Error {e} Skipping...')\n",
    "\n",
    "\n",
    "def optimize_hyperparameters_indirect_approach_first_step(encoder: object, predictor: object) -> None:\n",
    "    \"\"\"\n",
    "    Optimize hyperparameters for the first step of indirect approach: odor detection.\n",
    "    \n",
    "    This function optimizes encoder-predictor models for binary classification to predict whether\n",
    "    a molecule has any detectable odor (has_odor: 0 or 1).\n",
    "    \n",
    "    Args:\n",
    "        encoder (object): Molecular encoder class for molecular representation\n",
    "        predictor (object): Binary classifier for odor detection\n",
    "    \"\"\"\n",
    "    X = df_odor_strength['canonical_smiles'].values\n",
    "    y = df_odor_strength['has_odor'].values\n",
    "    stratify_data = df_odor_strength['numerical_strength'].values\n",
    "    X_train, _, y_train, _, groups_train, _ = stratified_group_train_test_split(X, y, groups, random_state=42, stratify_data=stratify_data)\n",
    "    X_train = X_train.tolist()\n",
    "    try:\n",
    "        encoder_name = encoder.__name__\n",
    "        predictor_name = predictor.__name__\n",
    "        if compatibility_check(encoder_name, predictor_name, w_regression=False):\n",
    "            print(f'Optimizing hyperparameters: {encoder_name} with {predictor_name}')\n",
    "            OdorStrengthHyperparameterOptimizer(\n",
    "                encoder,\n",
    "                predictor,\n",
    "                n_trials=N_TRIALS,\n",
    "                evaluation_metric=Metrics().calculate_f1_score,\n",
    "                groups=groups_train,\n",
    "                n_cv_splits=N_FOLDS,\n",
    "                n_cv_repeats=1,\n",
    "                load_if_exists=True,\n",
    "                predictor_hyperparameter_space_name_prefix='hyperparameter_space_binary_'\n",
    "                ).optimize_study(\n",
    "                    X=X_train,\n",
    "                    y=y_train,\n",
    "                    direction='maximize',\n",
    "                    study_name=f'{encoder.__name__}_{predictor.__name__}_indirect_first_step',\n",
    "                    storage_path=STORAGE_FOLDER + f'{encoder.__name__}_{predictor.__name__}_indirect_first_step.db',\n",
    "                    pruner=optuna.pruners.PercentilePruner(25, n_startup_trials=3),\n",
    "                    pruner_tolerance=0.015,\n",
    "                    n_repeats=N_REPEATS,\n",
    "                    callbacks=[callback]\n",
    "                    )\n",
    "    except Exception as e:\n",
    "        print(f'Error with {encoder_name} and {predictor_name}. Error {e} Skipping...')\n",
    "\n",
    "\n",
    "def optimize_hyperparameters_indirect_approach_second_step(encoder: object, predictor: object) -> None:\n",
    "    \"\"\"\n",
    "    Optimize hyperparameters for the second step of indirect approach: odor strength prediction.\n",
    "    \n",
    "    This function optimizes encoder-predictor models for regression to predict odor strength (1-3 scale)\n",
    "    for molecules that are odorous.\n",
    "    \n",
    "    Args:\n",
    "        encoder (object): Molecular encoder class for molecular representation\n",
    "        predictor (object): Regression model for odor strength prediction\n",
    "    \"\"\"\n",
    "    X = df_odor_strength['canonical_smiles'].values\n",
    "    y = df_odor_strength['numerical_strength'].values\n",
    "    X_train, _, y_train, _, groups_train, _ = stratified_group_train_test_split(X, y, groups, random_state=42)\n",
    "    X_train = X_train[y_train >= 1]\n",
    "    groups_train = groups_train[y_train >= 1]\n",
    "    y_train = y_train[y_train >= 1]\n",
    "    X_train = X_train.tolist()\n",
    "    try:\n",
    "        encoder_name = encoder.__name__\n",
    "        predictor_name = predictor.__name__\n",
    "        if compatibility_check(encoder_name, predictor_name, w_regression=True):\n",
    "            print(f'Optimizing hyperparameters: {encoder_name} with {predictor_name}')\n",
    "            OdorStrengthHyperparameterOptimizer(\n",
    "                encoder,\n",
    "                predictor,\n",
    "                n_trials=N_TRIALS,\n",
    "                evaluation_metric=Metrics().calculate_mse_macro,\n",
    "                groups=groups_train,\n",
    "                n_cv_splits=N_FOLDS,\n",
    "                n_cv_repeats=1,\n",
    "                load_if_exists=True,\n",
    "                predictor_hyperparameter_space_name_prefix='hyperparameter_space_'\n",
    "                ).optimize_study(\n",
    "                    X=X_train,\n",
    "                    y=y_train,\n",
    "                    direction='minimize',\n",
    "                    study_name=f'{encoder.__name__}_{predictor.__name__}_indirect_second_step',\n",
    "                    storage_path=STORAGE_FOLDER + f'{encoder.__name__}_{predictor.__name__}_indirect_second_step.db',\n",
    "                    pruner=optuna.pruners.PercentilePruner(25, n_startup_trials=3),\n",
    "                    pruner_tolerance=0.02,\n",
    "                    n_repeats=N_REPEATS,\n",
    "                    callbacks=[callback]\n",
    "                    )\n",
    "    except Exception as e:\n",
    "        print(f'Error with {encoder_name} and {predictor_name}. Error {e} Skipping...')\n",
    "              \n",
    "for encoder in encoders:\n",
    "    for predictor in predictors:\n",
    "        optimize_hyperparameters_direct_approach(encoder, predictor)\n",
    "        optimize_hyperparameters_indirect_approach_first_step(encoder, predictor)\n",
    "        optimize_hyperparameters_indirect_approach_second_step(encoder, predictor)\n",
    "print('Hyperparameter optimization completed for all combinations of encoders and predictors.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best-Performing Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import optuna\n",
    "\n",
    "from utility.colors import okabe_ito\n",
    "from models.predictors import Average, LogisticRegressionPredictor, RandomForestPredictor, XGBoostPredictor, MLP_Predictor, ChemPropMPNNPredictor, ChemeleonPredictor, CoralPredictor\n",
    "from models.molecule_encoder import NativeEncoder, MorganFp, RDKitFp, TopologicalTorsionFp, AtomPairFp, MACCSKeysFp, RDKitDescriptors, ChemBerta\n",
    "from hyperparameter_optimization.hyperparameter_optimizer import HyperparameterOptimizer\n",
    "from models.odor_strength_module import OdorStrengthModule, OdorStrengthModuleHyperparameterOptimizationWrapper\n",
    "\n",
    "\n",
    "\n",
    "DPI = 600\n",
    "FONTSIZE = 8\n",
    "LABELSIZE = 5\n",
    "LABELPAD = 3\n",
    "FIGURE_WIDTH = 8.3 / 2.54\n",
    "\n",
    "STORAGE_FOLDER = 'sqlite:///hyperparameter_optimization/hp_opt_dbs/'\n",
    "\n",
    "encoders = [\n",
    "    NativeEncoder,\n",
    "    RDKitDescriptors,\n",
    "    MorganFp,\n",
    "    RDKitFp,\n",
    "    TopologicalTorsionFp,\n",
    "    AtomPairFp,\n",
    "    MACCSKeysFp,\n",
    "    ChemBerta\n",
    "]\n",
    "predictors = [\n",
    "    RandomForestPredictor,\n",
    "    XGBoostPredictor,\n",
    "    MLP_Predictor,\n",
    "    LogisticRegressionPredictor,\n",
    "    CoralPredictor,\n",
    "    ChemPropMPNNPredictor,\n",
    "    ChemeleonPredictor,\n",
    "    ]\n",
    "\n",
    "X = df_odor_strength['canonical_smiles'].values\n",
    "y = df_odor_strength['numerical_strength'].values\n",
    "X_train, X_test, y_train, y_test, groups_train, groups_test = stratified_group_train_test_split(X, y, groups, random_state=42)\n",
    "X_train = X_train.tolist()\n",
    "X_test = X_test.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Evaluation Functions\n",
    "\n",
    "This section contains functions for evaluating the best-performing models from hyperparameter optimization and generating performance visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_test_set(\n",
    "        model_class: OdorStrengthModuleHyperparameterOptimizationWrapper,\n",
    "        hyperparameters: dict[str, Any],\n",
    "        X_train: list[str],\n",
    "        y_train: np.ndarray,\n",
    "        X_test: list[str],\n",
    "        y_test: np.ndarray,\n",
    "        repeats: int = 10\n",
    "        ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate predictions on the test set with multiple random initializations.\n",
    "    \n",
    "    This function trains the same model multiple times with different random seeds\n",
    "    to assess prediction robustness and provide uncertainty estimates.\n",
    "    \n",
    "    Args:\n",
    "        model_class: Class of the model to instantiate\n",
    "        hyperparameters (dict): Hyperparameters for model initialization\n",
    "        X_train (list): Training molecular SMILES strings\n",
    "        y_train (np.ndarray): Training target values\n",
    "        X_test (list): Test molecular SMILES strings\n",
    "        y_test (np.ndarray): Test target values\n",
    "        repeats (int): Number of training/prediction repeats (default: 10)\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with repeated predictions and corresponding true values\n",
    "    \"\"\"\n",
    "    preds_list = []\n",
    "    for i in range(repeats):\n",
    "        model = model_class(**hyperparameters)\n",
    "        # print('model initiated for prediction run', i+1)\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        preds_list.append(preds)\n",
    "    preds = np.hstack(preds_list)\n",
    "    y_test_stacked = np.hstack([y_test]*repeats)\n",
    "    df = pd.DataFrame({'y_test': y_test_stacked, 'preds': preds})\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_best_model_performances(\n",
    "        encoders: list[object],\n",
    "        predictors: list[object],\n",
    "        study_name_suffix: str,\n",
    "        storage_path_suffix: str,\n",
    "        direction: str,\n",
    "        metric: Callable[[np.ndarray, np.ndarray], float],\n",
    "        X_train: list[str],\n",
    "        y_train: np.ndarray,\n",
    "        X_test: list[str],\n",
    "        y_test: np.ndarray,\n",
    "        repeats: int = 10,\n",
    "        save_path_addition: str = '',\n",
    "        additional_test_set: tuple[list[str], np.ndarray] | None = None\n",
    "        ) -> tuple[pd.DataFrame, type, dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Evaluate the best models from hyperparameter optimization studies.\n",
    "    \n",
    "    This function loads optimized hyperparameter configurations from storage, evaluates them on test data,\n",
    "    and creates a performance matrix comparing all encoder-predictor combinations.\n",
    "    \n",
    "    Args:\n",
    "        encoders (list): List of molecular encoder classes\n",
    "        predictors (list): List of predictor classes\n",
    "        study_name_suffix (str): Suffix for study names to load\n",
    "        storage_path_suffix (str): Suffix for storage paths to load\n",
    "        direction (str): Optimization direction ('minimize' or 'maximize')\n",
    "        metric (callable): Evaluation metric function\n",
    "        X_train (list): Training molecular SMILES strings\n",
    "        y_train (np.ndarray): Training target values\n",
    "        X_test (list): Test molecular SMILES strings  \n",
    "        y_test (np.ndarray): Test target values\n",
    "        repeats (int): Number of evaluation repeats (default: 10)\n",
    "        save_path_addition (str): Additional path component for saving predictions\n",
    "        additional_test_set (tuple, optional): Additional test set (X, y) for evaluation\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Performance DataFrame, best model class, best hyperparameters\n",
    "    \"\"\"\n",
    "    encoder_name_list = [encoder.__name__ for encoder in encoders]\n",
    "    predictor_name_list = [predictor.__name__ for predictor in predictors]\n",
    "    df = pd.DataFrame(columns=encoder_name_list, index=predictor_name_list)\n",
    "    models = {}\n",
    "    hyperparameters = {}\n",
    "    for encoder in encoders:\n",
    "        for predictor in predictors:\n",
    "            study_name=f'{encoder.__name__}_{predictor.__name__}_' + study_name_suffix\n",
    "            storage_path=f'{STORAGE_FOLDER}{encoder.__name__}_{predictor.__name__}_' + storage_path_suffix + '.db'\n",
    "            try:\n",
    "                if not os.path.exists(storage_path.replace('sqlite:///', '')):\n",
    "                    storage_path = optuna.storages.JournalStorage(\n",
    "                        optuna.storages.journal.JournalFileBackend(\n",
    "                            STORAGE_FOLDER.replace('sqlite:///', '') + f'{encoder.__name__}_{predictor.__name__}_' + storage_path_suffix + '.log'\n",
    "                        )\n",
    "                    )   \n",
    "                study = optuna.load_study(\n",
    "                    study_name=study_name,\n",
    "                    storage=storage_path\n",
    "                )\n",
    "                complete_trials = [trial for trial in study.trials if trial.state == optuna.trial.TrialState.COMPLETE]\n",
    "                print(f'{storage_path_suffix} {encoder.__name__} {predictor.__name__} Number of complete trials: {len(complete_trials)}')\n",
    "                hp_opt = HyperparameterOptimizer(OdorStrengthModuleHyperparameterOptimizationWrapper, study)\n",
    "                if encoder.__name__ not in models:\n",
    "                    models[encoder.__name__] = {}\n",
    "                models[encoder.__name__][predictor.__name__] = hp_opt.model\n",
    "                if encoder.__name__ not in hyperparameters:\n",
    "                    hyperparameters[encoder.__name__] = {}\n",
    "                hyperparameters[encoder.__name__][predictor.__name__] = hp_opt.get_best_hyperparameters()\n",
    "                if not os.path.exists('test_predictions/'):\n",
    "                    os.makedirs('test_predictions/')\n",
    "                if not os.path.exists('test_predictions/' + save_path_addition + encoder.__name__ + predictor.__name__ + '_predictions.csv'):\n",
    "                    print('path not found, predicting on test set:', 'test_predictions/' + encoder.__name__ + predictor.__name__ + '_predictions.csv')\n",
    "                    df_pred = predict_on_test_set(hp_opt.model, hp_opt.get_best_hyperparameters(), X_train, y_train, X_test, y_test, repeats=repeats)\n",
    "                    df_pred.to_csv('test_predictions/' + save_path_addition + encoder.__name__ + predictor.__name__ + '_predictions.csv')\n",
    "                    print('saved successfully')\n",
    "                    if additional_test_set is not None:\n",
    "                        X_test_additional, y_test_additional = additional_test_set\n",
    "                        df_pred_additional = predict_on_test_set(hp_opt.model, hp_opt.get_best_hyperparameters(), X_train, y_train, X_test_additional, y_test_additional, repeats=repeats)\n",
    "                        df_pred_additional.to_csv('test_predictions/' + save_path_addition + encoder.__name__ + predictor.__name__ + '_predictions_additional.csv')\n",
    "                        print('saved additional test set successfully')\n",
    "                else:\n",
    "                    df_pred = pd.read_csv('test_predictions/' + save_path_addition + encoder.__name__ + predictor.__name__ + '_predictions.csv', index_col=0)\n",
    "                best_value_runs = []\n",
    "                for i in range(repeats):\n",
    "                    df_pred_subset = df_pred.iloc[i*len(y_test):(i+1)*len(y_test)]\n",
    "                    best_values = metric(df_pred_subset['y_test'].values, df_pred_subset['preds'].values)\n",
    "                    if isinstance(best_values, (list, tuple)):\n",
    "                        best_value = best_values[0]\n",
    "                    else:\n",
    "                        best_value = best_values\n",
    "                    best_value_runs.append(best_value)\n",
    "                best_value = np.mean(best_value_runs)\n",
    "                print('Standard deviation over runs for ' + encoder.__name__ + predictor.__name__, np.std(best_value_runs))\n",
    "                print(f'Best value for {study_name}: {best_value}')\n",
    "                df.loc[predictor.__name__, encoder.__name__] = best_value\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading study {study_name}: {e}\")\n",
    "                best_value = None\n",
    "    df = df.astype(float)\n",
    "    df[df<0] = np.nan\n",
    "    if direction == 'maximize':\n",
    "        best_position = np.unravel_index(df.fillna(-np.inf).values.argmax(), df.shape)\n",
    "    elif direction == 'minimize':\n",
    "        best_position = np.unravel_index(df.fillna(np.inf).values.argmin(), df.shape)\n",
    "    else:\n",
    "        raise ValueError(\"Direction must be 'maximize' or 'minimize'\")\n",
    "    row_label = df.index[best_position[0]]\n",
    "    col_label = df.columns[best_position[1]]\n",
    "    best_model = models[col_label][row_label]\n",
    "    best_hyperparameters = hyperparameters[col_label][row_label]\n",
    "    return df, best_model, best_hyperparameters\n",
    "\n",
    "\n",
    "encoders = [\n",
    "    NativeEncoder,\n",
    "    RDKitDescriptors,\n",
    "    MorganFp,\n",
    "    RDKitFp,\n",
    "    TopologicalTorsionFp,\n",
    "    AtomPairFp,\n",
    "    MACCSKeysFp,\n",
    "    ChemBerta\n",
    "]\n",
    "predictors = [\n",
    "    RandomForestPredictor,\n",
    "    XGBoostPredictor,\n",
    "    MLP_Predictor,\n",
    "    LogisticRegressionPredictor,\n",
    "    CoralPredictor,\n",
    "    ChemPropMPNNPredictor,\n",
    "    ChemeleonPredictor,\n",
    "    ]\n",
    "\n",
    "y_train_has_odor = y_train.copy()\n",
    "y_train_has_odor[y_train_has_odor>0] = 1\n",
    "y_test_has_odor = y_test.copy()\n",
    "y_test_has_odor[y_test_has_odor>0] = 1\n",
    "X_train_wo_odorless = np.array(X_train)[np.array(y_train)>0].tolist()\n",
    "y_train_wo_odorless = y_train.copy()[np.array(y_train)>0]\n",
    "X_test_wo_odorless = np.array(X_test)[np.array(y_test)>0].tolist()\n",
    "y_test_wo_odorless = y_test.copy()[np.array(y_test)>0]\n",
    "\n",
    "df_direct, best_model_direct_class, best_hyperparameters_direct = get_best_model_performances(\n",
    "    encoders,\n",
    "    predictors,\n",
    "    'direct',\n",
    "    'direct',\n",
    "    direction='minimize',\n",
    "    metric=Metrics().calculate_mse_macro,\n",
    "    X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test,\n",
    "    save_path_addition='direct'\n",
    "    )\n",
    "df_indirect_1, best_model_indirect_first_step_class, best_hyperparameters_indirect_first_step = get_best_model_performances(\n",
    "    encoders,\n",
    "    predictors,\n",
    "    'indirect_first_step',\n",
    "    'indirect_first_step',\n",
    "    direction='maximize',\n",
    "    metric=Metrics().calculate_f1_score,\n",
    "    X_train=X_train, y_train=y_train_has_odor, X_test=X_test, y_test=y_test_has_odor,\n",
    "    save_path_addition='indirect_1'\n",
    "    )\n",
    "df_indirect_2, best_model_indirect_second_step_class, best_hyperparameters_indirect_second_step = get_best_model_performances(\n",
    "    encoders,\n",
    "    predictors, \n",
    "    'indirect_second_step',\n",
    "    'indirect_second_step',\n",
    "     direction='minimize',\n",
    "     metric=Metrics().calculate_mse_macro,\n",
    "     X_train=X_train_wo_odorless, y_train=y_train_wo_odorless, X_test=X_test_wo_odorless, y_test=y_test_wo_odorless,\n",
    "     save_path_addition='indirect_2',\n",
    "     additional_test_set=(X_test, y_test)\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indirect Approach: Combining Two-Step Predictions\n",
    "\n",
    "This section combines the predictions from the two-step indirect approach:\n",
    "1. First model predicts if a molecule has any odor (binary classification)\n",
    "2. Second model predicts odor strength for molecules classified as odorous (regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeats = 10\n",
    "y_test_stack = np.hstack([y_test]*repeats)\n",
    "save_path_addition_1 = 'indirect_1'\n",
    "save_path_addition_2 = 'indirect_2'\n",
    "mse_macros = {encoder.__name__: {} for encoder in encoders}\n",
    "\n",
    "for encoder in encoders:\n",
    "    for predictor in predictors:\n",
    "        try:\n",
    "            df_pred_1 = pd.read_csv('test_predictions/' + save_path_addition_1 + encoder.__name__ + predictor.__name__ + '_predictions.csv', index_col=0)\n",
    "            df_pred_2 = pd.read_csv('test_predictions/' + save_path_addition_2 + encoder.__name__ + predictor.__name__ + '_predictions_additional.csv', index_col=0)\n",
    "            df_pred_combined = df_pred_2.copy()\n",
    "            df_pred_combined.loc[df_pred_1['preds'] < 0.5, 'preds'] = df_pred_1.loc[df_pred_1['preds'] < 0.5, 'preds']\n",
    "            mse_macros[encoder.__name__][predictor.__name__] = Metrics().calculate_mse_macro(y_test_stack, df_pred_combined['preds'].values)[0]\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing predictions for {encoder.__name__} {predictor.__name__}: {e}\")\n",
    "            continue\n",
    "df_odor_strength_indirect = pd.DataFrame(mse_macros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_odor_strength_indirect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "DPI = 600\n",
    "colors = [okabe_ito[1], okabe_ito[3]]\n",
    "mycmap = LinearSegmentedColormap.from_list(\"mycmap\", colors, N=256)\n",
    "width_factor = np.sqrt(2.45/2.2)\n",
    "\n",
    "\n",
    "def plot_heatmap(\n",
    "        df: pd.DataFrame,\n",
    "        colorbar_label: str,\n",
    "        figsize: Tuple[int, int] = (10, 10),\n",
    "        fontsize: int = 12,\n",
    "        labelsize: int = 12,\n",
    "        labelpad: int = 10,\n",
    "        dpi: int = 100,\n",
    "        save_path: list[str] | None = None,\n",
    "        custom_x_ticks: list[str] | None = None,\n",
    "        custom_y_ticks: list[str] | None = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Create a performance heatmap for encoder-predictor combinations.\n",
    "    \n",
    "    This function visualizes model performance across different combinations of\n",
    "    molecular encoders and predictors using a color-coded heatmap.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Performance matrix with predictors as rows and encoders as columns\n",
    "        colorbar_label (str): Label for the colorbar indicating the performance metric\n",
    "        figsize (Tuple[int, int]): Figure size in inches (width, height)\n",
    "        fontsize (int): Font size for labels and titles\n",
    "        labelsize (int): Font size for tick labels\n",
    "        labelpad (int): Padding for axis labels\n",
    "        dpi (int): Resolution for saved figures\n",
    "        save_path (list[str], optional): List of file paths to save the figure\n",
    "        custom_x_ticks (list[str], optional): Custom labels for x-axis (encoders)\n",
    "        custom_y_ticks (list[str], optional): Custom labels for y-axis (predictors)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(df, cmap=mycmap, aspect='auto')\n",
    "    colorbar = plt.colorbar()\n",
    "    colorbar.set_label(colorbar_label, fontsize=fontsize, labelpad=labelpad+labelpad*0.2)\n",
    "    colorbar.ax.tick_params(labelsize=labelsize)\n",
    "    # plt.xlabel('Encoders', fontsize=fontsize, labelpad=labelpad)\n",
    "    # plt.ylabel('Predictors', fontsize=fontsize, labelpad=labelpad)\n",
    "    if custom_x_ticks is None:\n",
    "        custom_x_ticks = [label.replace('Encoder', '') for label in df.columns]\n",
    "    plt.xticks(ticks=range(len(custom_x_ticks)), labels=custom_x_ticks, rotation=45, ha='right')\n",
    "    if custom_y_ticks is None:\n",
    "        custom_y_ticks = [label.split('Predictor')[0] for label in df.index]\n",
    "    plt.yticks(ticks=range(len(df.index)), labels=custom_y_ticks)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=labelsize)\n",
    "    plt.tight_layout()\n",
    "    if save_path is not None:\n",
    "        for path in save_path:\n",
    "            plt.savefig(path, dpi=dpi, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_heatmap(\n",
    "    df_direct.dropna(how='all').iloc[:4, 1:],\n",
    "    colorbar_label='F1 Score',\n",
    "    figsize=(FIGURE_WIDTH*1.2, 2*1.2),\n",
    "    dpi=DPI,\n",
    "    fontsize=FONTSIZE,\n",
    "    labelsize=LABELSIZE,\n",
    "    labelpad=LABELPAD,\n",
    "    save_path=['figures/hp_opt_heatmap_has_odor.pdf', 'figures/hp_opt_heatmap_has_odor.png'],\n",
    "    custom_x_ticks=['RDKit Descriptors', 'Morgan FP', 'RDKit FP', 'Topological Torsion FP', 'Atom Pair FP', 'MACCCS Keys FP', 'ChemBERTa'],\n",
    "    custom_y_ticks=['Logistic Regression', 'Random Forest', 'XGBoost', 'MLP']\n",
    "    )\n",
    "plot_heatmap(\n",
    "    - df_indirect_1.dropna(how='all').iloc[:5, 1:],\n",
    "    colorbar_label='Negative Macro MSE',\n",
    "    figsize=(FIGURE_WIDTH*1.2, 2.2*1.2),\n",
    "    dpi=DPI,\n",
    "    fontsize=FONTSIZE,\n",
    "    labelsize=LABELSIZE,\n",
    "    labelpad=LABELPAD,\n",
    "    save_path=['figures/hp_opt_heatmap_wo_odorless.pdf', 'figures/hp_opt_heatmap_wo_odorless.png'],\n",
    "    custom_x_ticks=['RDKit Descriptors', 'Morgan FP', 'RDKit FP', 'Topological Torsion FP', 'Atom Pair FP', 'MACCCS Keys FP', 'ChemBERTa'],\n",
    "    custom_y_ticks=['Logistic Regression', 'Random Forest', 'XGBoost', 'MLP', 'CORAL']\n",
    "    )\n",
    "plot_heatmap(\n",
    "    - df_odor_strength_indirect.iloc[2:, 1:],\n",
    "    colorbar_label='Negative Macro MSE',\n",
    "    figsize=(FIGURE_WIDTH*1.2, 2.2*1.2),\n",
    "    dpi=DPI,\n",
    "    fontsize=FONTSIZE,\n",
    "    labelsize=LABELSIZE,\n",
    "    labelpad=LABELPAD,\n",
    "    save_path=['figures/hp_opt_heatmap_indirect.pdf', 'figures/hp_opt_heatmap_indirect.png'],\n",
    "    custom_x_ticks=['RDKit Descriptors', 'Morgan FP', 'RDKit FP', 'Topological Torsion FP', 'Atom Pair FP', 'MACCCS Keys FP', 'ChemBERTa'],\n",
    "    custom_y_ticks=['Logistic Regression', 'Random Forest', 'XGBoost', 'MLP']\n",
    "    )\n",
    "# \n",
    "plot_heatmap(\n",
    "    - df_odor_strength_indirect.dropna(how='all').iloc[:5, 1:],\n",
    "    colorbar_label=' Negative Macro MSE',\n",
    "    figsize=(FIGURE_WIDTH*width_factor, 2.2*width_factor),\n",
    "    dpi=DPI,\n",
    "    fontsize=FONTSIZE,\n",
    "    labelsize=LABELSIZE,\n",
    "    labelpad=LABELPAD,\n",
    "    save_path=['figures/hp_opt_heatmap_w_odorless.pdf', 'figures/hp_opt_heatmap_w_odorless.png'],\n",
    "    custom_x_ticks=['RDKit Descriptors', 'Morgan FP', 'RDKit FP', 'Topological Torsion FP', 'Atom Pair FP', 'MACCCS Keys FP', 'ChemBERTa'],\n",
    "    custom_y_ticks=['Logistic Regression', 'Random Forest', 'XGBoost', 'MLP', 'CORAL']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison: Direct vs. Indirect Approaches\n",
    "\n",
    "This section compares the best-performing models from both approaches on the test set to determine which strategy works better for odor strength prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeats = 10\n",
    "preds_list = []\n",
    "preds_two_step_list = []\n",
    "\n",
    "X = df_odor_strength['canonical_smiles'].values\n",
    "y = df_odor_strength['has_odor'].values\n",
    "stratify_data = df_odor_strength['numerical_strength'].values\n",
    "X_train, X_test, y_train, y_test, groups_train, groups_test = stratified_group_train_test_split(X, stratify_data, groups, random_state=42)\n",
    "X_train = X_train.tolist()\n",
    "X_test = X_test.tolist()\n",
    "\n",
    "\n",
    "for i in range(repeats):\n",
    "    model = best_model_direct_class(**best_hyperparameters_direct)\n",
    "    model.fit(X_train, y_train)\n",
    "#     model.save(\n",
    "#     encoder_path='encoder.gz',\n",
    "#     predictor_path='predictor.pth',\n",
    "#     predictor_hyperparameter_path='predictor_hyperparameters.json',\n",
    "#     config_path='model_config.txt',\n",
    "# )\n",
    "    preds = model.predict(X_test)\n",
    "    preds_list.append(preds)\n",
    "\n",
    "\n",
    "    indirect_first_step_model = best_model_indirect_first_step_class(**best_hyperparameters_indirect_first_step)\n",
    "    indirect_second_step_model = best_model_indirect_second_step_class(**best_hyperparameters_indirect_second_step)\n",
    "\n",
    "    y_train_has_odor = y_train.copy()\n",
    "    y_train_has_odor[y_train_has_odor>0] = 1\n",
    "    indirect_first_step_model.fit(X_train, y_train_has_odor)\n",
    "\n",
    "    y_train_wo_odorless = y_train.copy()\n",
    "    X_train_wo_odorless = np.array(X_train)[y_train_wo_odorless > 1].tolist()\n",
    "    y_train_wo_odorless = y_train_wo_odorless[y_train_wo_odorless > 1]\n",
    "    indirect_second_step_model.fit(X_train_wo_odorless, y_train_wo_odorless)\n",
    "\n",
    "    preds_two_step = indirect_first_step_model.predict(X_test)\n",
    "    preds_two_step[preds_two_step>0.5] = indirect_second_step_model.predict(np.array(X_test)[preds_two_step>0.5].tolist())\n",
    "    preds_two_step_list.append(preds_two_step)\n",
    "\n",
    "preds = np.hstack(preds_list)\n",
    "preds_two_step = np.hstack(preds_two_step_list)\n",
    "y_test = np.hstack([y_test]*repeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from utility.colors import okabe_ito\n",
    "\n",
    "def plot_confusion_matrix(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    figsize: tuple = (10, 10),\n",
    "    dpi: int = 100,\n",
    "    tick_labels: dict = {0: \"Odorless\", 1: \"Low\", 2: \"Medium\", 3: \"High\"},\n",
    "    cmap: str|LinearSegmentedColormap = 'viridis',\n",
    "    text_annotation: bool = True,\n",
    "    fontsize: int = 20,\n",
    "    labelsize: int = 18,\n",
    "    labelpad: int = 10,\n",
    "    save_path: str | None = None\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Plots a normalized (by true values) confusion matrix for classification results.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.ndarray\n",
    "        Array of true class labels.\n",
    "    y_pred : np.ndarray\n",
    "        Array of predicted class labels.\n",
    "    figsize : tuple, optional\n",
    "        Size of the figure in inches (width, height). Default is (10, 10).\n",
    "    dpi : int, optional\n",
    "        Dots per inch for the figure resolution. Default is 100.\n",
    "    tick_labels : dict, optional\n",
    "        Dictionary mapping class indices to label names for axis ticks. If prediction or true values are higher or lower than the keys in this dictionary, they will be set to the maximum or minimum key value, respectively.\n",
    "        Default is {0: \"None\", 1: \"Low\", 2: \"Medium\", 3: \"High\"}.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Displays the confusion matrix plot.\n",
    "    \"\"\"\n",
    "    y_pred = y_pred.round().astype(int)\n",
    "    y_true = y_true.round().astype(int)\n",
    "    max_tick_value = max(tick_labels.keys())\n",
    "    min_tick_value = min(tick_labels.keys())\n",
    "    y_pred[y_pred > max_tick_value], y_pred[y_pred < min_tick_value] = max_tick_value, min_tick_value\n",
    "    y_true[y_true > max_tick_value], y_true[y_true < min_tick_value] = max_tick_value, min_tick_value\n",
    "    confusion = confusion_matrix(y_true, y_pred)\n",
    "    confusion = confusion.T\n",
    "    normed_confusion_matrix = confusion / np.sum(confusion, axis=0)\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    im = ax.imshow(normed_confusion_matrix, cmap=cmap, aspect='equal', origin='lower')\n",
    "    if text_annotation:\n",
    "        for i in range(normed_confusion_matrix.shape[0]):\n",
    "            for j in range(normed_confusion_matrix.shape[1]):\n",
    "                text = ax.text(\n",
    "                    j, i, f\"{normed_confusion_matrix[i, j]:.2f}\",\n",
    "                    ha='center', va='center',\n",
    "                    color='white' # if normed_confusion_matrix[i, j] < 0.5 else 'black'  # adjust for contrast\n",
    "                )\n",
    "    colorbar = plt.colorbar(im)\n",
    "    colorbar.set_label('Ratio of test values', fontsize=fontsize, labelpad=labelpad+labelpad*0.2)\n",
    "    colorbar.ax.tick_params(labelsize=labelsize)\n",
    "    plt.xticks(list(tick_labels.keys()), list(tick_labels.values()))\n",
    "    plt.yticks(list(tick_labels.keys()), list(tick_labels.values()))\n",
    "    plt.tick_params(axis='both', which='major', labelsize=labelsize)\n",
    "    plt.xlabel('Test Set', fontsize=fontsize, labelpad=labelpad)\n",
    "    plt.ylabel('Prediction', fontsize=fontsize, labelpad=labelpad)\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=dpi, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "colors = [okabe_ito[4], okabe_ito[5]]\n",
    "mycmap = LinearSegmentedColormap.from_list(\"mycmap\", colors, N=256)\n",
    "\n",
    "print(Metrics().calculate_mse_macro(y_test, preds), Metrics().calculate_mse(y_test, preds))\n",
    "samples_per_repeat = int(y_test.shape[0]/repeats)\n",
    "mse_macros = [Metrics().calculate_mse_macro(y_test[i*samples_per_repeat:(i+1)*samples_per_repeat], preds[i*samples_per_repeat:(i+1)*samples_per_repeat])[0] for i in range(repeats)]\n",
    "print('Standard MSE Macro Deviation per repeat', np.std(mse_macros))\n",
    "print(mse_macros)\n",
    "print(Metrics().calculate_r2(y_test, preds))\n",
    "r2s = [Metrics().calculate_r2(y_test[i*samples_per_repeat:(i+1)*samples_per_repeat], preds[i*samples_per_repeat:(i+1)*samples_per_repeat]) for i in range(repeats)]\n",
    "print('Standard R2 Deviation per repeat', np.std(r2s))\n",
    "plot_confusion_matrix(\n",
    "    y_test,\n",
    "    preds,\n",
    "    cmap=mycmap,\n",
    "    figsize=(FIGURE_WIDTH, 2.45),\n",
    "    dpi=DPI,\n",
    "    text_annotation=False,\n",
    "    fontsize=FONTSIZE,\n",
    "    labelsize=LABELSIZE,\n",
    "    labelpad=LABELPAD,\n",
    "    )\n",
    "\n",
    "print(Metrics().calculate_mse_macro(y_test, preds_two_step), Metrics().calculate_mse(y_test, preds_two_step))\n",
    "mse_macros_two_step = [Metrics().calculate_mse_macro(y_test[i*samples_per_repeat:(i+1)*samples_per_repeat], preds_two_step[i*samples_per_repeat:(i+1)*samples_per_repeat])[0] for i in range(repeats)]\n",
    "print('Standard MSE Macro Deviation per repeat', np.std(mse_macros_two_step))\n",
    "print(mse_macros_two_step)\n",
    "print(Metrics().calculate_r2(y_test, preds_two_step))\n",
    "r2s_two_step = [Metrics().calculate_r2(y_test[i*samples_per_repeat:(i+1)*samples_per_repeat], preds_two_step[i*samples_per_repeat:(i+1)*samples_per_repeat]) for i in range(repeats)]\n",
    "print('Standard R2 Deviation per repeat', np.std(r2s_two_step))\n",
    "plot_confusion_matrix(\n",
    "    y_test,\n",
    "    preds_two_step,\n",
    "    cmap=mycmap,\n",
    "    figsize=(FIGURE_WIDTH, 3),\n",
    "    dpi=DPI,\n",
    "    text_annotation=False,\n",
    "    fontsize=FONTSIZE,\n",
    "    labelsize=LABELSIZE,\n",
    "    labelpad=LABELPAD,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.data_cleaner import GoodScentsDataCleaner\n",
    "import pandas as pd\n",
    "\n",
    "keller_2016 = pd.read_excel('data/keller_2016/12868_2016_287_MOESM1_ESM.xlsx', header=2)\n",
    "keller_2016.rename(columns={'C.A.S.': 'cas'}, inplace=True)\n",
    "data_cleaner = GoodScentsDataCleaner(data=keller_2016)\n",
    "data_cleaner.clean_molecules()\n",
    "keller_2016 = data_cleaner.data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External Validation on Keller 2016 Dataset\n",
    "\n",
    "This section validates the best-performing model on an independent dataset from Keller et al. (2016) to assess generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keller_2016 = keller_2016[keller_2016['canonical_smiles'].notna()]\n",
    "keller_2016['predicted_intensity'] = model.predict(keller_2016['canonical_smiles'].tolist())\n",
    "keller_2016['predicted_intensity_rounded'] = keller_2016['predicted_intensity'].round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keller_2016_test  = keller_2016[~keller_2016['canonical_smiles'].isin(df_odor_strength['canonical_smiles'])]\n",
    "from tqdm import tqdm\n",
    "def calculate_morgan_tanimoto_similarity(smiles_list_1: list[str], smiles_list_2: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Check for molecular similarity between training and external test sets.\n",
    "    \n",
    "    This function computes Tanimoto similarity coefficients using Morgan fingerprints\n",
    "    to identify molecules in the external test set that are too similar to training\n",
    "    molecules, which could lead to data leakage.\n",
    "    \n",
    "    Args:\n",
    "        smiles_list_1 (list): SMILES strings from training dataset\n",
    "        smiles_list_2 (list): SMILES strings from external test dataset\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Similarity matrix with external molecules as rows and \n",
    "                     training molecules as columns\n",
    "    \"\"\"\n",
    "    morgan_fingerprint = MorganFp(radius=3, fpSize=2048)\n",
    "    morgan_fingerprints_1 = morgan_fingerprint.encode(smiles_list_1)\n",
    "    morgan_fingerprints_2 = morgan_fingerprint.encode(smiles_list_2)\n",
    "    # morgan_similarities = {smiles_1: np.sum(np.logical_and(np.array(morgan_fingerprints_2), fp1), axis=1) / len(fp1) for smiles_1, fp1 in tqdm(zip(smiles_list_1, morgan_fingerprints_1))}\n",
    "    morgan_similarities = {smiles_1: np.sum(np.logical_and(np.array(morgan_fingerprints_2), fp1), axis=1) / np.sum(np.logical_or(np.array(morgan_fingerprints_2), fp1), axis=1) for smiles_1, fp1 in tqdm(zip(smiles_list_1, morgan_fingerprints_1))}\n",
    "\n",
    "    # morgan_similarities = {smiles_2: [jaccard_score(fp1, fp2) for smiles_1, fp1 in zip(smiles_list_1, morgan_fingerprints_1)] for smiles_2, fp2 in tqdm(zip(smiles_list_2, morgan_fingerprints_2))}\n",
    "    df_morgan_similarities = pd.DataFrame(morgan_similarities, index=smiles_list_2)\n",
    "    return df_morgan_similarities\n",
    "df_morgan_similarities = calculate_morgan_tanimoto_similarity(df_odor_strength['canonical_smiles'].tolist(), keller_2016_test['canonical_smiles'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_to_remove = df_morgan_similarities[df_morgan_similarities.max(axis=1) > 0.8].index\n",
    "keller_2016_test = keller_2016_test[~keller_2016_test['canonical_smiles'].isin(smiles_to_remove)]\n",
    "keller_2016_test['canonical_smiles'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keller_2016_test['Odor dilution'] = keller_2016_test['Odor dilution'].map({  \n",
    "    '1/10': 0.1,\n",
    "    '1/1,000': 0.001,\n",
    "    '1/100,000': 0.00001,\n",
    "    '1/10,000,000': 1e-07\n",
    "}).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate standard deviation per dilution averaged over molecules\n",
    "print(keller_2016_test.groupby('canonical_smiles')['HOW STRONG IS THE SMELL?'].count().mean())\n",
    "\n",
    "for dilution in keller_2016_test['Odor dilution'].unique():\n",
    "    print(f'Number of molecules: {keller_2016_test[keller_2016_test['Odor dilution'] == dilution]['canonical_smiles'].nunique()}')\n",
    "    groups = keller_2016_test[keller_2016_test['Odor dilution'] == dilution].groupby('canonical_smiles')\n",
    "    std_dev = groups['HOW STRONG IS THE SMELL?'].std()\n",
    "    print(f'Std dev for dilution {dilution} per molecule: mean: {std_dev.mean()}, std: {std_dev.std()}, min: {std_dev.min()}, max: {std_dev.max()}')\n",
    "    count = groups['HOW STRONG IS THE SMELL?'].count()\n",
    "    print(f'Count for dilution {dilution} per molecule: mean: {count.mean()}, std: {count.std()}, min: {count.min()}, max: {count.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "from utility.colors import okabe_ito\n",
    "\n",
    "FIGURE_WIDTH_LONG = 17.1 / 2.54\n",
    "\n",
    "def plot_combined_figure_with_violins(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    df: pd.DataFrame,\n",
    "    keller_data: pd.DataFrame,\n",
    "    colorbar_label_1: str,\n",
    "    colorbar_label_2: str,\n",
    "    tick_labels: dict = {0: \"Odorless\", 1: \"Low\", 2: \"Medium\", 3: \"High\"},\n",
    "    text_annotation: bool = True,\n",
    "    custom_x_ticks: list[str] | None = None,\n",
    "    custom_y_ticks: list[str] | None = None,\n",
    "    figsize: tuple = (15, 18),\n",
    "    dpi: int = 100,\n",
    "    fontsize: int = 12,\n",
    "    labelsize: int = 12,\n",
    "    labelpad: int = 10,\n",
    "    cmap: str|LinearSegmentedColormap = 'viridis',\n",
    "    save_path: list[str] | None = None,\n",
    "    width_ratios: list[float] = [1, 1, 1, 1, 1, 1],\n",
    "    height_ratios: list[float] = [1, 1],\n",
    "    hspace: float = 1\n",
    "    ) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Create a comprehensive multi-panel figure combining performance visualization.\n",
    "    \n",
    "    This function creates a complex figure with multiple subplots:\n",
    "    - Top left: Performance heatmap across encoder-predictor combinations\n",
    "    - Top right: Confusion matrix for the best model\n",
    "    - Bottom: Violin plots showing external validation results\n",
    "    \n",
    "    Args:\n",
    "        y_true (np.ndarray): True labels for confusion matrix\n",
    "        y_pred (np.ndarray): Predicted labels for confusion matrix\n",
    "        df (pd.DataFrame): Performance matrix for heatmap\n",
    "        keller_data (pd.DataFrame): External validation dataset\n",
    "        colorbar_label_1 (str): Label for heatmap colorbar\n",
    "        colorbar_label_2 (str): Label for confusion matrix colorbar\n",
    "        tick_labels (dict): Mapping of numeric labels to descriptive names\n",
    "        text_annotation (bool): Whether to add text annotations to confusion matrix\n",
    "        custom_x_ticks (list[str], optional): Custom x-axis labels for heatmap\n",
    "        custom_y_ticks (list[str], optional): Custom y-axis labels for heatmap\n",
    "        figsize (tuple): Figure size in inches\n",
    "        dpi (int): Resolution for saved figures\n",
    "        fontsize (int): Font size for labels\n",
    "        labelsize (int): Font size for tick labels\n",
    "        labelpad (int): Padding for axis labels\n",
    "        cmap: Colormap for visualizations\n",
    "        save_path (list[str], optional): Paths to save the figure\n",
    "        width_ratios (list[float]): Relative widths of subplot columns\n",
    "        height_ratios (list[float]): Relative heights of subplot rows\n",
    "        hspace (float): Height spacing between subplots\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Matplotlib figure object\n",
    "    \"\"\"\n",
    "\n",
    "    # Create figure and grid\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "\n",
    "    # Create a more structured grid: 2 rows, 6 columns with different width ratios for top row\n",
    "    gs = fig.add_gridspec(2, 6, height_ratios=height_ratios, \n",
    "                         width_ratios=width_ratios,\n",
    "                         hspace=hspace, wspace=0.7)\n",
    "    \n",
    "    # Top row: Heatmap (left 3 cols) and Confusion matrix (right 2 cols)\n",
    "    ax_heatmap = fig.add_subplot(gs[0, :2])\n",
    "    ax_confusion = fig.add_subplot(gs[0, 3:])\n",
    "    \n",
    "    # --- HEATMAP ---\n",
    "    im1 = ax_heatmap.imshow(df, cmap=cmap)\n",
    "    \n",
    "    if custom_x_ticks is None:\n",
    "        custom_x_ticks = [label.replace('Encoder', '') for label in df.columns]\n",
    "    ax_heatmap.set_xticks(range(len(custom_x_ticks)))\n",
    "    ax_heatmap.set_xticklabels(custom_x_ticks, rotation=45, ha='right')\n",
    "    if custom_y_ticks is None:\n",
    "        custom_y_ticks = [label.split('Predictor')[0] for label in df.index]\n",
    "    ax_heatmap.set_yticks(range(len(df.index)))\n",
    "    ax_heatmap.set_yticklabels(custom_y_ticks)\n",
    "    ax_heatmap.tick_params(axis='both', which='major', labelsize=labelsize)\n",
    "    \n",
    "    # --- CONFUSION MATRIX ---\n",
    "    y_pred_rounded = y_pred.round().astype(int)\n",
    "    y_true_rounded = y_true.round().astype(int)\n",
    "    max_tick_value = max(tick_labels.keys())\n",
    "    min_tick_value = min(tick_labels.keys())\n",
    "    y_pred_rounded[y_pred_rounded > max_tick_value], y_pred_rounded[y_pred_rounded < min_tick_value] = max_tick_value, min_tick_value\n",
    "    y_true_rounded[y_true_rounded > max_tick_value], y_true_rounded[y_true_rounded < min_tick_value] = max_tick_value, min_tick_value\n",
    "    confusion = confusion_matrix(y_true_rounded, y_pred_rounded)\n",
    "    confusion = confusion.T\n",
    "    normed_confusion_matrix = confusion / np.sum(confusion, axis=0)\n",
    "    \n",
    "    im2 = ax_confusion.imshow(normed_confusion_matrix, cmap=cmap, origin='lower')\n",
    "    if text_annotation:\n",
    "        for i in range(normed_confusion_matrix.shape[0]):\n",
    "            for j in range(normed_confusion_matrix.shape[1]):\n",
    "                text = ax_confusion.text(\n",
    "                    j, i, f\"{normed_confusion_matrix[i, j]:.2f}\",\n",
    "                    ha='center', va='center',\n",
    "                    color='white'\n",
    "                )\n",
    "    \n",
    "    ax_confusion.set_xticks(list(tick_labels.keys()))\n",
    "    ax_confusion.set_xticklabels(list(tick_labels.values()))\n",
    "    ax_confusion.set_yticks(list(tick_labels.keys()))\n",
    "    ax_confusion.set_yticklabels(list(tick_labels.values()))\n",
    "    ax_confusion.tick_params(axis='both', which='major', labelsize=labelsize)\n",
    "    ax_confusion.set_xlabel('Test Set', fontsize=fontsize, labelpad=labelpad)\n",
    "    ax_confusion.set_ylabel('Prediction', fontsize=fontsize, labelpad=labelpad)\n",
    "    \n",
    "    cax1 = ax_heatmap.inset_axes([1.05, 0, 0.04, 1])  # relative to the parent Axes\n",
    "    colorbar1 = plt.colorbar(im1, cax=cax1) \n",
    "    colorbar1.set_label(colorbar_label_1, fontsize=fontsize, labelpad=labelpad+labelpad*0.2)    # Manually adjust subplot positions to ensure equal visual height\n",
    "    colorbar1.ax.tick_params(labelsize=labelsize)\n",
    "\n",
    "    cax2 = ax_confusion.inset_axes([1.05, 0, 0.05, 1])  # relative to the parent Axes\n",
    "    colorbar2 = plt.colorbar(im2, cax=cax2) \n",
    "    colorbar2.set_label(colorbar_label_2, fontsize=fontsize, labelpad=labelpad+labelpad*0.2)    # Manually adjust subplot positions to ensure equal visual height\n",
    "    colorbar2.ax.tick_params(labelsize=labelsize)\n",
    "\n",
    "    pos1 = ax_heatmap.get_position()\n",
    "    pos2 = ax_confusion.get_position()\n",
    "    \n",
    "    ax_heatmap.set_position([pos1.x0, 1 - pos1.height, pos1.width, pos1.height])\n",
    "    ax_confusion.set_position([pos2.x0, 1 - pos2.height, pos2.width, pos2.height])\n",
    "    \n",
    "    ax_heatmap.text(-0.55, 1.08, 'a', transform=ax_heatmap.transAxes, \n",
    "            fontsize=fontsize+2, fontweight='bold', ha='right', va='top')\n",
    "    ax_confusion.text(-0.35, 1.06, 'b', transform=ax_confusion.transAxes, \n",
    "        fontsize=fontsize+2, fontweight='bold', ha='right', va='top')\n",
    "\n",
    "    violin_axes = [\n",
    "        fig.add_subplot(gs[1, :3]),  # Row 1, cols 0-2\n",
    "        fig.add_subplot(gs[1, 3:]),  # Row 1, cols 3-5\n",
    "    ]\n",
    "    \n",
    "    violin_labels = ['c', 'd']\n",
    "    dilutions = [1.e-03, 1.e-05]\n",
    "    \n",
    "    colors_4 = {\n",
    "        0.0: okabe_ito[2],\n",
    "        1.0: okabe_ito[3],\n",
    "        2.0: okabe_ito[1],\n",
    "        3.0: okabe_ito[7]\n",
    "        }\n",
    "    \n",
    "    for i, ax_violin in enumerate(violin_axes):\n",
    "        if i == 0:        \n",
    "            subset = keller_data[keller_data['Odor dilution'] == dilutions[i]]\n",
    "            print(f'Number of compounds:', subset['canonical_smiles'].nunique())\n",
    "            sns.violinplot(data=subset, x='predicted_intensity_rounded', y='HOW STRONG IS THE SMELL?', \n",
    "                        ax=ax_violin, hue='predicted_intensity_rounded', legend=False, palette=colors_4,\n",
    "                        cut=0,\n",
    "                        density_norm='area',\n",
    "                        common_norm=True,\n",
    "                        order=[0.0, 1.0, 2.0, 3.0]\n",
    "                        )\n",
    "            \n",
    "            \n",
    "            ax_violin.set_xlim(-0.5, 3.5)\n",
    "            if i % 2 == 0:\n",
    "                ax_violin.set_ylabel('Rated Intensity', fontsize=fontsize)\n",
    "            else:\n",
    "                ax_violin.set_ylabel('')\n",
    "                ax_violin.tick_params(labelleft=False)\n",
    "            ax_violin.set_xlabel('Predicted Odor Strength', fontsize=fontsize)\n",
    "            ax_violin.tick_params(axis='both', which='major', labelsize=labelsize)\n",
    "            ax_violin.set_xticks(list(tick_labels.keys()))\n",
    "            ax_violin.set_xticklabels(list(tick_labels.values()))\n",
    "        if i % 2 == 0:\n",
    "            text_x = -0.16\n",
    "        else:\n",
    "            text_x = -0.03\n",
    "        ax_violin.text(text_x, 1.1, violin_labels[i], transform=ax_violin.transAxes, \n",
    "                      fontsize=fontsize+2, fontweight='bold', ha='right', va='top')\n",
    "    violin_positions = [ax.get_position() for ax in violin_axes]\n",
    "    y_shift = 0.16 \n",
    "    for i, ax_violin in enumerate(violin_axes):\n",
    "        pos = violin_positions[i]\n",
    "        if i % 2 == 0:\n",
    "            new_width = pos.width * 1.15  \n",
    "            new_x = pos.x0 - 0.07 \n",
    "        else:\n",
    "            new_width = pos.width * 1.15\n",
    "            new_x = pos.x0 -0.02 \n",
    "        \n",
    "        if i < 2: \n",
    "            new_pos = [new_x, pos.y0 + y_shift, new_width, pos.height]\n",
    "        else: \n",
    "            new_pos = [new_x, pos.y0 + 1.37*y_shift, new_width, pos.height]\n",
    "        ax_violin.set_position(new_pos)  \n",
    "    plt.tight_layout()\n",
    "    if save_path is not None:\n",
    "        for path in save_path:\n",
    "            plt.savefig(path, dpi=dpi, bbox_inches='tight', pad_inches=0.1)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "colors = [okabe_ito[1], okabe_ito[3]]\n",
    "mycmap = LinearSegmentedColormap.from_list(\"mycmap\", colors, N=256)\n",
    "\n",
    "combined_fig = plot_combined_figure_with_violins(\n",
    "    y_true=y_test_stack,\n",
    "    y_pred=preds,\n",
    "    df=-df_direct.dropna(how='all').iloc[:5, 1:],\n",
    "    keller_data=keller_2016_test,\n",
    "    colorbar_label_1=' Negativ Macro MSE',\n",
    "    colorbar_label_2='Ratio of Test Categories',\n",
    "    tick_labels={0: \"Odorless\", 1: \"Low\", 2: \"Medium\", 3: \"High\"},\n",
    "    text_annotation=False,\n",
    "    custom_x_ticks=['RDKit Descriptors', 'Morgan FP', 'RDKit FP', 'Topological Torsion FP', 'Atom Pair FP', 'MACCCS Keys FP', 'ChemBERTa'],\n",
    "    custom_y_ticks=['Logistic Regression', 'Random Forest', 'XGBoost', 'MLP', 'CORAL'],\n",
    "    figsize=(FIGURE_WIDTH_LONG*1.07, FIGURE_WIDTH_LONG * 0.78*1.07),\n",
    "    dpi=DPI,\n",
    "    fontsize=FONTSIZE,\n",
    "    labelsize=LABELSIZE,\n",
    "    labelpad=LABELPAD,\n",
    "    cmap=mycmap,\n",
    "    save_path=['figures/combined_performance_plots.pdf', 'figures/combined_performance_plots.svg', 'figures/combined_performance_plots.png'],\n",
    "    width_ratios=[1, 1, 0.4, 1, 1, 0.4],\n",
    "    height_ratios=[1, 0.85],\n",
    "    hspace=0.4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from utility.colors import okabe_ito\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "colors = [okabe_ito[1], okabe_ito[0], okabe_ito[3]]\n",
    "mycmap = LinearSegmentedColormap.from_list(\"mycmap\", colors, N=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP Analysis Setup\n",
    "\n",
    "Setting up SHAP (SHapley Additive exPlanations) analysis to understand feature importance and model interpretability for the best-performing odor strength prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_X_train = model.odor_strength_module.molecule_encoder.encode(X_train)\n",
    "encoded_X_test = model.odor_strength_module.molecule_encoder.encode(X_test)\n",
    "feature_names = encoded_X_test.columns if hasattr(encoded_X_test, 'columns') else [f'Feature {i}' for i in range(encoded_X_test.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape and type of encoded data\n",
    "print(\"Encoded X_train shape:\", encoded_X_train.shape)\n",
    "print(\"Encoded X_train type:\", type(encoded_X_train))\n",
    "print(\"Encoded X_test shape:\", encoded_X_test.shape)\n",
    "\n",
    "# Convert to numpy arrays if they're pandas DataFrames\n",
    "\n",
    "if hasattr(encoded_X_train, 'values'):\n",
    "    encoded_X_train_np = encoded_X_train.values\n",
    "    encoded_X_test_np = encoded_X_test.values\n",
    "else:\n",
    "    encoded_X_train_np = encoded_X_train\n",
    "    encoded_X_test_np = encoded_X_test\n",
    "\n",
    "print(\"Final shapes - Train:\", encoded_X_train_np.shape, \"Test:\", encoded_X_test_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a wrapper function for the predictor that works with encoded features\n",
    "def predictor_wrapper(encoded_features: np.ndarray | pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Wrapper function for SHAP analysis that takes encoded molecular features as input.\n",
    "    \n",
    "    This wrapper is necessary for SHAP to properly interface with the predictor\n",
    "    component of the odor strength module, bypassing the encoding step.\n",
    "    \n",
    "    Args:\n",
    "        encoded_features (np.ndarray or pd.DataFrame): Pre-encoded molecular features\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Predicted odor strength values\n",
    "    \"\"\"\n",
    "\n",
    "    predictions = model.odor_strength_module.odor_strength_predictor.predict(encoded_features.values if hasattr(encoded_features, 'values') else encoded_features)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Test the wrapper\n",
    "test_pred = predictor_wrapper(encoded_X_train_np[:5])\n",
    "print(\"Test prediction shape:\", test_pred.shape)\n",
    "print(\"Test predictions:\", test_pred.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model-agnostic SHAP analysis using encoded features\n",
    "\n",
    "# Use a subset of training data as background for faster computation\n",
    "background_sample_size = encoded_X_train.shape[0]\n",
    "test_sample_size = encoded_X_test.shape[0]\n",
    "\n",
    "# Create the SHAP explainer using the predictor wrapper\n",
    "explainer = shap.Explainer(predictor_wrapper, encoded_X_train_np[:background_sample_size], feature_names=feature_names)\n",
    "\n",
    "shap_values = explainer(encoded_X_test_np[:test_sample_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Feature Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix\n",
    "\n",
    "encoded_X =  model.odor_strength_module.molecule_encoder.encode(df_odor_strength['canonical_smiles'].values)\n",
    "correlation_matrix = encoded_X.corr().fillna(0)\n",
    "\n",
    "fig = plt.figure(figsize=(FIGURE_WIDTH_LONG, FIGURE_WIDTH_LONG))\n",
    "plt.imshow(correlation_matrix, cmap=mycmap, vmin=-1, vmax=1)\n",
    "plt.colorbar(label='Correlation Coefficient', shrink=0.73)\n",
    "plt.xticks(ticks=np.arange(len(feature_names)), labels=feature_names, rotation=90, fontsize=2)\n",
    "plt.yticks(ticks=np.arange(len(feature_names)), labels=feature_names, fontsize=2)\n",
    "# plt.title('Feature Correlation Matrix', fontsize=10)\n",
    "plt.tight_layout()\n",
    "print(fig.get_size_inches()*2.54)\n",
    "plt.savefig('figures/feature_correlation_matrix.pdf', dpi=DPI)\n",
    "plt.savefig('figures/feature_correlation_matrix.svg')\n",
    "# plt.tick_params(labelbottom=False, labelleft=False)\n",
    "# plt.xticks(ticks=[])\n",
    "# plt.yticks(ticks=[])\n",
    "plt.savefig('figures/feature_correlation_matrix.png', dpi=DPI)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved feature grouping using agglomerative clustering\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import squareform\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agglomerative clustering\n",
    "\n",
    "correlation_matrix = pd.DataFrame(encoded_X_train_np).corr()\n",
    "feature_variances = np.var(encoded_X_train_np, axis=0)\n",
    "distance_matrix = 1 - np.abs(correlation_matrix.fillna(0))\n",
    "correlation_threshold = 0.75\n",
    "distance_threshold = 1 - correlation_threshold\n",
    "\n",
    "clustering = AgglomerativeClustering(\n",
    "    n_clusters=None, \n",
    "    distance_threshold=distance_threshold,\n",
    "    linkage='average', \n",
    "    metric='precomputed'\n",
    ")\n",
    "\n",
    "# Fit the clustering\n",
    "cluster_labels = clustering.fit_predict(distance_matrix)\n",
    "\n",
    "print(f\"Agglomerative clustering created {clustering.n_clusters_} clusters\")\n",
    "\n",
    "# Create feature groups based on cluster labels\n",
    "feature_groups = []\n",
    "for cluster_id in range(clustering.n_clusters_):\n",
    "    cluster_features = [i for i, label in enumerate(cluster_labels) if label == cluster_id]\n",
    "    feature_groups.append(cluster_features)\n",
    "\n",
    "# Select representative features for each group (highest variance)\n",
    "selected_features_idx = []\n",
    "selected_features_names = []\n",
    "cluster_info = []\n",
    "\n",
    "for group_id, feature_indices in enumerate(feature_groups):\n",
    "    if len(feature_indices) > 1:\n",
    "        # Multiple features in group - select the one with highest variance\n",
    "        group_variances = [(idx, feature_names[idx], feature_variances[idx]) for idx in feature_indices]\n",
    "        best_feature = max(group_variances, key=lambda x: x[2])\n",
    "        selected_features_idx.append(best_feature[0])\n",
    "        selected_features_names.append(best_feature[1])\n",
    "        cluster_info.append({\n",
    "            'cluster_id': group_id,\n",
    "            'representative': best_feature[1],\n",
    "            'members': [feature_names[idx] for idx in feature_indices],\n",
    "            'size': len(feature_indices)\n",
    "        })\n",
    "    else:\n",
    "        idx = feature_indices[0]\n",
    "        name = feature_names[idx]\n",
    "        selected_features_idx.append(idx)\n",
    "        selected_features_names.append(name)\n",
    "        cluster_info.append({\n",
    "            'cluster_id': group_id,\n",
    "            'representative': name,\n",
    "            'members': [name],\n",
    "            'size': 1\n",
    "        })\n",
    "\n",
    "print(f\"Agglomerative clustering: Selected {len(selected_features_idx)} representative features\")\n",
    "print(f\"Reduced from {len(feature_names)} to {len(selected_features_idx)} features ({100*(1-len(selected_features_idx)/len(feature_names)):.1f}% reduction)\")\n",
    "\n",
    "multi_member_groups = [c for c in cluster_info if c['size'] > 1]\n",
    "print(f\"\\nAgglomerative clustering - Groups with multiple correlated features ({len(multi_member_groups)} groups):\")\n",
    "for group in sorted(multi_member_groups, key=lambda x: x['size'], reverse=True)[:15]:\n",
    "    print(f\"  Cluster {group['cluster_id']} ({group['size']} features): {group['representative']} represents {group['members']}{'...' if len(group['members']) > 5 else ''}\")\n",
    "\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"Greedy approach: {len(selected_features_idx)} representative features, {len([c for c in cluster_info if c['size'] > 1])} multi-member groups\")\n",
    "print(f\"Agglomerative: {len(selected_features_idx)} representative features, {len(multi_member_groups)} multi-member groups\")\n",
    "\n",
    "# Show cluster size distributions\n",
    "greedy_sizes = [c['size'] for c in cluster_info if c['size'] > 1]\n",
    "agg_sizes = [c['size'] for c in cluster_info if c['size'] > 1]\n",
    "\n",
    "print(f\"\\nCluster size statistics:\")\n",
    "print(f\"Greedy - Max: {max(greedy_sizes) if greedy_sizes else 0}, Mean: {np.mean(greedy_sizes) if greedy_sizes else 0:.1f}, Median: {np.median(greedy_sizes) if greedy_sizes else 0:.1f}\")\n",
    "print(f\"Agglomerative - Max: {max(agg_sizes) if agg_sizes else 0}, Mean: {np.mean(agg_sizes) if agg_sizes else 0:.1f}, Median: {np.median(agg_sizes) if agg_sizes else 0:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global SHAP feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from utility.colors import okabe_ito\n",
    "import matplotlib as mpl\n",
    "\n",
    "bar_color = okabe_ito[3]\n",
    "\n",
    "clustered_shap_values = np.zeros((shap_values.values.shape[0], len(feature_groups)))\n",
    "for cluster_id, feature_indices in enumerate(feature_groups):\n",
    "    clustered_shap_values[:, cluster_id] = np.sum(np.abs(shap_values.values[:, feature_indices]), axis=1)\n",
    "clustered_shap = shap.Explanation(values=clustered_shap_values,\n",
    "                                #  base_values=clustered_shap_values.mean(axis=0),\n",
    "                                base_values=np.array([0.0]*len(feature_groups)),\n",
    "                                 data=None,\n",
    "                                 feature_names=selected_features_names)\n",
    "\n",
    "fig = plt.figure(figsize=(FIGURE_WIDTH, FIGURE_WIDTH*0.75))\n",
    "shap.plots.bar(clustered_shap,\n",
    "                    show=False,\n",
    "                    max_display=6,\n",
    "                    )\n",
    "ax = plt.gca()\n",
    "for txt in list(ax.texts):\n",
    "    try:\n",
    "        txt.remove()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "patches = [p for p in ax.patches if isinstance(p, mpl.patches.Rectangle) and p.get_width() > 0]\n",
    "\n",
    "num_bars = len(patches)\n",
    "if num_bars > 0:\n",
    "    for i, p in enumerate(patches):\n",
    "        p.set_facecolor(bar_color)\n",
    "\n",
    "y_tick_locs = ax.get_yticks()\n",
    "y_tick_labels = [t.get_text() for t in ax.get_yticklabels()]\n",
    "new_y_tick_labels = ['Polarity', 'Weight and Shape', 'Rings', 'Alcohol Groups', 'Branching', f'{133 - len(y_tick_labels)} other Groups']\n",
    "if len(new_y_tick_labels) == len(y_tick_labels) // 2:\n",
    "    ax.set_yticklabels(2*new_y_tick_labels)\n",
    "\n",
    "current_size = fig.get_size_inches()\n",
    "new_width = FIGURE_WIDTH\n",
    "scale_factor = new_width / current_size[0]\n",
    "new_height = current_size[1] * scale_factor\n",
    "fig.set_size_inches(new_width, new_height)\n",
    "ax.set_xlabel('Sum absolute SHAP values', fontsize=FONTSIZE, labelpad=LABELPAD)\n",
    "ax.tick_params(axis='both', which='major', labelsize=LABELSIZE)\n",
    "print(fig.get_size_inches()*2.54)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/shap_feature_importance.pdf', dpi=DPI)\n",
    "plt.savefig('figures/shap_feature_importance.svg')\n",
    "plt.savefig('figures/shap_feature_importance.png', dpi=DPI)\n",
    "plt.show()\n",
    "\n",
    "print(\"Y tick labels:\", y_tick_labels)\n",
    "print(fig.get_size_inches()*2.54)\n",
    "\n",
    "\n",
    "df_clustered_shap_values = pd.DataFrame(clustered_shap_values, columns=selected_features_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "colors = [okabe_ito[3], okabe_ito[1]]\n",
    "mycmap = LinearSegmentedColormap.from_list(\"mycmap\", colors, N=256)\n",
    "\n",
    "\n",
    "def plot_summary_plot_feature_group(most_important_feature: str) -> None:\n",
    "    \"\"\"\n",
    "    Create a SHAP summary plot for the most important feature group.\n",
    "    \n",
    "    This function generates a beeswarm plot showing SHAP values for all features\n",
    "    within the most important feature groups, colored by feature values.\n",
    "    \n",
    "    Args:\n",
    "        most_important_feature (str): Name of the representative feature for the cluster\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Representative feature name and list of cluster feature names\n",
    "    \"\"\"\n",
    "    most_important_cluster = next(c for c in cluster_info if c['representative'] == most_important_feature)\n",
    "\n",
    "    print(f\"Most important feature group:\")\n",
    "    print(f\"Representative feature: {most_important_cluster['representative']}\")\n",
    "    print(f\"Cluster size: {most_important_cluster['size']} features\")\n",
    "    print(f\"All features in this cluster: {most_important_cluster['members']}\")\n",
    "    most_important_cluster_indices = [i for i, name in enumerate(feature_names) if name in most_important_cluster['members']]\n",
    "    cluster_shap_values = shap_values.values[:, most_important_cluster_indices]\n",
    "    cluster_feature_values = encoded_X_test_np[:test_sample_size, most_important_cluster_indices]\n",
    "    cluster_feature_names = [feature_names[i] for i in most_important_cluster_indices]\n",
    "    cluster_shap_explanation = shap.Explanation(\n",
    "        values=cluster_shap_values,\n",
    "        base_values=shap_values.base_values,\n",
    "        data=cluster_feature_values,\n",
    "        feature_names=cluster_feature_names\n",
    "    )\n",
    "\n",
    "    fig =plt.figure(figsize=(FIGURE_WIDTH, FIGURE_WIDTH*0.75))\n",
    "    shap.plots.beeswarm(cluster_shap_explanation,\n",
    "                        max_display=20,\n",
    "                        show=False)\n",
    "    cmap = mpl.cm.get_cmap(mycmap)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    for coll in ax.collections:\n",
    "        try:\n",
    "            coll.set_cmap(cmap)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # update any image artists / colorbars if present\n",
    "    for im in ax.get_images():\n",
    "        try:\n",
    "            im.set_cmap(cmap)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # redraw figure\n",
    "    plt.draw()\n",
    "    updated_cbar = False\n",
    "    for coll in ax.collections:\n",
    "        try:\n",
    "            arr = coll.get_array()\n",
    "            if arr is None or len(arr) == 0:\n",
    "                continue\n",
    "            norm = getattr(coll, \"norm\", None)\n",
    "            mappable = mpl.cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "            mappable.set_array(arr)\n",
    "\n",
    "            for a in fig.axes[:]:\n",
    "                if a is not ax:\n",
    "                    pos = a.get_position()\n",
    "                    if pos.width < 0.2 or pos.height < 0.2:\n",
    "                        try:\n",
    "                            fig.delaxes(a)\n",
    "                        except Exception:\n",
    "                            pass\n",
    "            cbar = fig.colorbar(mappable, ax=ax, orientation=\"vertical\", pad=0.02)\n",
    "            cbar.set_label(\"Feature Value\", rotation=270, labelpad=12, fontsize=FONTSIZE)\n",
    "            # remove colorbar tick labels\n",
    "            try:\n",
    "                cbar.set_ticks([])\n",
    "                cbar.ax.set_yticklabels([])\n",
    "            except Exception:\n",
    "                pass\n",
    "            updated_cbar = True\n",
    "            break\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    ax.set_xlabel('SHAP Value (Impact on Model Output)', fontsize=FONTSIZE, labelpad=LABELPAD)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=LABELSIZE)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'figures/shap_summary_plot_{most_important_cluster[\"representative\"]}_cluster.pdf', dpi=DPI)\n",
    "    plt.show()\n",
    "\n",
    "important_features = []\n",
    "representative_features = []\n",
    "\n",
    "for representative_feature in y_tick_labels[:5]:\n",
    "    rep_feature, cluster_features = plot_summary_plot_feature_group(representative_feature)\n",
    "    representative_features.append(rep_feature)\n",
    "    important_features.extend(cluster_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_dict = { # representative: group\n",
    "    'TPSA': 'Polarity',\n",
    "    'MolWt': 'Weight and Shape',\n",
    "    'NumAtomStereoCenters': 'Rings',\n",
    "    'PEOE_VSA10': 'Alcohol Groups',\n",
    "    'Kappa2': 'Branching'\n",
    "    }\n",
    "\n",
    "df_clusterd_shap_values_renamed = df_clustered_shap_values.rename(columns=lambda x: groups_dict.get(x, x))\n",
    "\n",
    "category_colors = [okabe_ito[2], okabe_ito[3], okabe_ito[1], okabe_ito[7]]\n",
    "\n",
    "preds_rounded = np.round(model.predict(X_test)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2x2 subplot for each category showing mean SHAP values\n",
    "fig, axes = plt.subplots(2, 2, figsize=(FIGURE_WIDTH_LONG, FIGURE_WIDTH*1.5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "subplot_labels = ['a', 'b', 'c', 'd']\n",
    "for i, category in enumerate([0, 1, 2, 3]):\n",
    "    category_mask = preds_rounded == category\n",
    "    category_shap_data = df_clusterd_shap_values_renamed[category_mask]\n",
    "    mean_shap_values = category_shap_data[groups_dict.values()].mean()\n",
    "    bars = axes[i].barh(range(len(mean_shap_values)), mean_shap_values.values, color=category_colors[i], alpha=0.7)\n",
    "    axes[i].set_xlim(0, 0.6)\n",
    "    if i % 2 == 0:\n",
    "        axes[i].set_yticks(range(len(mean_shap_values)))\n",
    "        axes[i].set_yticklabels(mean_shap_values.index, fontsize=LABELSIZE)\n",
    "    else:\n",
    "        axes[i].set_yticks(range(len(mean_shap_values)))\n",
    "        axes[i].set_yticklabels([])\n",
    "    if i > 1:\n",
    "        axes[i].set_xlabel('Mean Absolute SHAP Value', fontsize=FONTSIZE)\n",
    "        axes[i].tick_params(axis='x', which='major', labelsize=LABELSIZE)\n",
    "    else:\n",
    "        axes[i].set_xticklabels([])\n",
    "    axes[i].invert_yaxis()\n",
    "    if i % 2 == 0:\n",
    "        text_x = -0.4\n",
    "    else:\n",
    "        text_x = -0.03\n",
    "    axes[i].text(text_x, 1.1, f'{subplot_labels[i]}', transform=axes[i].transAxes, \n",
    "        fontsize=FONTSIZE+2, fontweight='bold', ha='right', va='top')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/shap_feature_importance_by_category.pdf', dpi=DPI)\n",
    "plt.savefig('figures/shap_feature_importance_by_category.svg')\n",
    "plt.savefig('figures/shap_feature_importance_by_category.png', dpi=DPI)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_explain(smiles: str, background: pd.DataFrame | np.ndarray, save_path: str | None = None, dpi: int = DPI) -> None:\n",
    "    \"\"\"\n",
    "    Generate and visualize local SHAP explanations for individual molecules.\n",
    "    \n",
    "    This function creates a waterfall plot showing how each feature group contributes\n",
    "    to the prediction for a specific molecule, starting from the base value.\n",
    "    \n",
    "    Args:\n",
    "        smiles (str): SMILES string of the molecule to explain\n",
    "        background (pd.DataFrame or np.ndarray): Background dataset for SHAP explainer\n",
    "        save_path (str, optional): Path to save the visualization\n",
    "        dpi (int): Resolution for saved figures\n",
    "    \"\"\"\n",
    "    max_display = 6\n",
    "    encoded_mol = model.odor_strength_module.molecule_encoder.encode([smiles])\n",
    "    encoded_mol_np = encoded_mol.values if hasattr(encoded_mol, 'values') else encoded_mol\n",
    "    background_values = background.values if hasattr(background, 'values') else background\n",
    "\n",
    "    explainer = shap.Explainer(predictor_wrapper, background_values, feature_names=background.columns.tolist() if hasattr(background, 'columns') else None)\n",
    "    shap_values_local = explainer(encoded_mol_np)\n",
    "    clustered_shap_values = np.zeros((shap_values_local.values.shape[0], len(feature_groups)))\n",
    "    for cluster_id, feature_indices in enumerate(feature_groups):\n",
    "        clustered_shap_values[:, cluster_id] = np.sum(shap_values_local.values[:, feature_indices], axis=1)\n",
    "    selected_clusters = pd.DataFrame(clustered_shap_values, columns=selected_features_names)\n",
    "    selected_clusters = selected_clusters[representative_features]\n",
    "    mapping = {\n",
    "        'TPSA': 'Polarity',\n",
    "        'MolWt': 'Weight and Shape',\n",
    "        'NumAtomStereoCenters': 'Rings',\n",
    "        'PEOE_VSA10': 'Alcohol Groups',\n",
    "        'Kappa2': 'Branching',\n",
    "    }\n",
    "    selected_clusters.columns = [mapping.get(col, col) for col in selected_clusters.columns]\n",
    "    other_clusters =  clustered_shap_values.sum() - selected_clusters.values.sum()\n",
    "    clustered_shap_values = selected_clusters.values\n",
    "    print(y.mean(axis=None))\n",
    "    print(shap_values_local.base_values)\n",
    "    \n",
    "    shap_values_clustered = shap.Explanation(values=clustered_shap_values,\n",
    "                                    base_values=shap_values_local.base_values + other_clusters,\n",
    "                                    data=None,\n",
    "                                    feature_names=selected_clusters.columns.tolist())\n",
    "\n",
    "\n",
    "    print(f\"SHAP values for molecule: {smiles}\")\n",
    "    fig = plt.figure(figsize=(FIGURE_WIDTH, FIGURE_WIDTH*0.75))\n",
    "    shap.plots.waterfall(shap_values_clustered[0], max_display=max_display, show=False)\n",
    "    for ax in fig.get_axes():\n",
    "        for txt in list(ax.texts):\n",
    "            txt.set_visible(False)\n",
    "        for child in ax.get_children():\n",
    "            if hasattr(child, 'get_text'):\n",
    "                try:\n",
    "                    child.set_visible(False)\n",
    "                except:\n",
    "                    pass\n",
    "            elif hasattr(child, '__class__') and 'Arrow' in str(child.__class__):\n",
    "                color = child.get_edgecolor()\n",
    "                if color[0] == 1:\n",
    "                    child.set_color(okabe_ito[3])\n",
    "                else:\n",
    "                    child.set_color(okabe_ito[1])\n",
    "                pass\n",
    "        \n",
    "        ax.set_title('')\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel('')\n",
    "    \n",
    "    for ax in fig.get_axes():\n",
    "        plt.setp(ax.get_xticklabels(), fontsize=LABELSIZE*2.25)\n",
    "        plt.setp(ax.get_yticklabels(), fontsize=LABELSIZE*2.25)\n",
    "        \n",
    "        ax.tick_params(axis='both', which='major', labelsize=LABELSIZE*2.25)\n",
    "    \n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, dpi=DPI)\n",
    "    \n",
    "    factor = 2\n",
    "    for ax in fig.get_axes():\n",
    "        plt.setp(ax.get_xticklabels(), fontsize=factor*LABELSIZE)\n",
    "        plt.setp(ax.get_yticklabels(), fontsize=factor*LABELSIZE)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=factor*LABELSIZE)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path.split('.')[0] + '.png', dpi=DPI)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "local_explain('OCCO', background=encoded_X_train, save_path='figures/shap_local_explanation_ethylene_glycol_group.svg', dpi=None)\n",
    "local_explain('CC(C)=CCC/C(C)=C\\CC/C(C)=C/CO', background=encoded_X_train, save_path='figures/shap_local_explanation_ez_farnesol_group.svg', dpi=None)\n",
    "local_explain('CC1COCc2cc3c(cc21)C(C)(C)C(C)C3(C)C\t', background=encoded_X_train, save_path='figures/shap_local_explanation_galaxolide_group.svg', dpi=None)\n",
    "local_explain('c1ccc2[nH]ccc2c1', background=encoded_X_train, save_path='figures/shap_local_explanation_indole_group.svg', dpi=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "odor_strength_code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
